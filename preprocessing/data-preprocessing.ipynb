{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import keras\n",
    "from keras.utils.image_utils import img_to_array, array_to_img, load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STARE\n",
    "def data_augmentation():\n",
    "    img_dir = '../data/filtered-data/ODIR/Pure AMD' #orginal directory\n",
    "    aug_dir = '../data/filtered-data/ODIR/Pure AMD/Augmented' #augmented directory\n",
    "    data_path = os.path.join(img_dir,'*g')\n",
    "\n",
    "    files = glob.glob(data_path) #error @ rgb_img when removing glob.glob dunno y\n",
    "    data = []\n",
    "    for f1 in files:\n",
    "        img = cv2.imread(f1) #read img\n",
    "        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #convert BGR img to RGB (depends)\n",
    "        data.append(rgb_img) \n",
    "\n",
    "        x = img_to_array(rgb_img)  \n",
    "        x = x.reshape((1,) + x.shape)  \n",
    "\n",
    "        path, dirs, files = next(os.walk(img_dir))\n",
    "        file_count = len(files)\n",
    "\n",
    "        datagen = ImageDataGenerator(horizontal_flip = True)\n",
    "\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size = 1, save_to_dir = aug_dir, save_prefix = 'ODIR_Augment', save_format = 'jpg'):\n",
    "            i += 1\n",
    "            if i == 1:\n",
    "                break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Channel Extraction & CLAHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_amd = []\n",
    "normal = []\n",
    "amd_w_others = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rce_clahe(img_dir, red_dir, data_family, data_class):\n",
    "    if not os.path.exists(red_dir):\n",
    "        os.mkdir(red_dir)\n",
    "\n",
    "    datagen = ImageDataGenerator(horizontal_flip = True)\n",
    "\n",
    "    i = 0\n",
    "    for images in os.listdir(img_dir): \n",
    "        # check if the image ends with png or jpg or jpeg\n",
    "        if (images.endswith(\".png\") or images.endswith(\".jpg\")\\\n",
    "            or images.endswith(\".jpeg\")):\n",
    "            img = cv2.imread(img_dir + \"/\" + images, cv2.IMREAD_UNCHANGED)\n",
    "            resized_img = cv2.resize(img, (512, 512))\n",
    "\n",
    "            # exclude red channel\n",
    "            b,g,r = cv2.split(resized_img)\n",
    "\n",
    "            # apply CLAHE\n",
    "            clahe = cv2.createCLAHE(clipLimit=5)\n",
    "            b_clahe_enhanced = clahe.apply(b) + 30 # increase intensity of contrast\n",
    "            g_clahe_enhanced = clahe.apply(g) + 30\n",
    "            r_clahe_enhanced = clahe.apply(r)\n",
    "\n",
    "            # merge enhanced channels\n",
    "            clahe_enhanced_img = cv2.merge((b_clahe_enhanced, g, r_clahe_enhanced))\n",
    "\n",
    "            grayscale = cv2.cvtColor(clahe_enhanced_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            if data_class == 'pure_amd':\n",
    "                pure_amd.append(grayscale)\n",
    "            elif data_class == 'normal':\n",
    "                normal.append(grayscale)\n",
    "            elif data_class == 'amd_w_others':\n",
    "                amd_w_others.append(grayscale)\n",
    "\n",
    "            path = os.path.join(red_dir, \"{}_CLAHE_\".format(data_family) + images.split('.')[0] + \".png\")\n",
    "            cv2.imwrite(path , grayscale)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rce_clahe('../data/preprocessing/filtered-data/ODIR/Pure AMD', '../data/preprocessing/CLAHE/ODIR/Pure AMD', 'ODIR', 'pure_amd')\n",
    "#rce_clahe('../data/preprocessing/filtered-data/ODIR/AMD w others', '../data/preprocessing/CLAHE/ODIR/AMD w others', 'ODIR', 'amd_w_others')\n",
    "#rce_clahe('../data/preprocessing/filtered-data/ODIR/Normal', '../data/preprocessing/CLAHE/ODIR/Normal', 'ODIR', 'normal')\n",
    "\n",
    "#rce_clahe('../data/preprocessing/filtered-data/RFMiD/Pure AMD', '../data/CLAHE/RFMiD/Pure AMD', 'RFMiD', 'pure_amd')\n",
    "#rce_clahe('../data/preprocessing/filtered-data/RFMiD/AMD w others', '../data/preprocessing/CLAHE/RFMiD/AMD w others', 'RFMiD', 'amd_w_others')\n",
    "#rce_clahe('../data/preprocessing/filtered-data/RFMiD/Normal', '../data/preprocessing/CLAHE/RFMiD/Normal', 'RFMiD', 'normal')\n",
    "#\n",
    "rce_clahe('../data/preprocessing/filtered-data/STARE/AMD w others', '../data/preprocessing/CLAHE/STARE/AMD w others', 'QSTARE', 'amd_w_others')\n",
    "#rce_clahe('../data/preprocessing/filtered-data/STARE/Normal', '../data/preprocessing/CLAHE/STARE/Normal', 'STARE', 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(img_dir, data_class):\n",
    "    for images in os.listdir(img_dir): \n",
    "        # check if the image ends with png or jpg or jpeg\n",
    "        if (images.endswith(\".png\") or images.endswith(\".jpg\")\\\n",
    "            or images.endswith(\".jpeg\")):\n",
    "            img = cv2.imread(img_dir + \"/\" + images, cv2.IMREAD_UNCHANGED)\n",
    "            resized_img = cv2.resize(img, (512, 512))\n",
    "\n",
    "            # exclude red channel\n",
    "            b,g,r = cv2.split(resized_img)\n",
    "\n",
    "            # apply CLAHE\n",
    "            clahe = cv2.createCLAHE(clipLimit=5)\n",
    "            b_clahe_enhanced = clahe.apply(b) + 30 # increase intensity of contrast\n",
    "            g_clahe_enhanced = clahe.apply(g) + 30\n",
    "            r_clahe_enhanced = clahe.apply(r)\n",
    "\n",
    "            # merge enhanced channels\n",
    "            clahe_enhanced_img = cv2.merge((b_clahe_enhanced, g, r_clahe_enhanced))\n",
    "\n",
    "            grayscale = cv2.cvtColor(clahe_enhanced_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            if data_class == 'normal':\n",
    "                train_data.append([grayscale, 0])\n",
    "            elif data_class == 'pure_amd':\n",
    "                train_data.append([grayscale, 1])\n",
    "            elif data_class == 'amd_w_others':\n",
    "                train_data.append([grayscale, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data('../data/preprocessing/filtered-data/ODIR/Pure AMD', 'pure_amd')\n",
    "#training_data('../data/preprocessing/filtered-data/ODIR/AMD w others', 'amd_w_others')\n",
    "#training_data('../data/preprocessing/filtered-data/ODIR/Normal', 'normal')\n",
    "\n",
    "#training_data('../data/preprocessing/filtered-data/RFMiD/Pure AMD', 'pure_amd')\n",
    "#training_data('../data/preprocessing/filtered-data/RFMiD/AMD w others', 'amd_w_others')\n",
    "#training_data('../data/preprocessing/filtered-data/RFMiD/Normal', 'normal')\n",
    "\n",
    "#training_data('../data/preprocessing/filtered-data/STARE/AMD w others', 'amd_w_others')\n",
    "#training_data('../data/preprocessing/filtered-data/STARE/Normal', 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for images, labels in train_data:\n",
    "    x.append(images)\n",
    "    y.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Data\n",
    "import pickle\n",
    "\n",
    "pickle_out = open(\"../data/pickles/x.pickle\",\"wb\")\n",
    "pickle.dump(x, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"../data/pickles/y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "958b22caee2cb0ccb656ab37bd85ad373cd98d377de0a0db81d7ef881a779b94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
