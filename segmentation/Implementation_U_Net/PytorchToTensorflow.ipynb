{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mackr\\anaconda3\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "import tensorflow as tf\n",
    "\n",
    "from MSUnet_Model import MSU_Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pytorch = MSU_Net(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_pytorch.state_dict(), 'model_MSUnet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 512, 512])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 512, 512]              80\n",
      "       BatchNorm2d-2          [-1, 8, 512, 512]              16\n",
      "              ReLU-3          [-1, 8, 512, 512]               0\n",
      "            Conv2d-4          [-1, 8, 512, 512]             584\n",
      "       BatchNorm2d-5          [-1, 8, 512, 512]              16\n",
      "              ReLU-6          [-1, 8, 512, 512]               0\n",
      "      conv_block_3-7          [-1, 8, 512, 512]               0\n",
      "            Conv2d-8          [-1, 8, 512, 512]             400\n",
      "       BatchNorm2d-9          [-1, 8, 512, 512]              16\n",
      "             ReLU-10          [-1, 8, 512, 512]               0\n",
      "           Conv2d-11          [-1, 8, 512, 512]           3,144\n",
      "      BatchNorm2d-12          [-1, 8, 512, 512]              16\n",
      "             ReLU-13          [-1, 8, 512, 512]               0\n",
      "     conv_block_7-14          [-1, 8, 512, 512]               0\n",
      "           Conv2d-15          [-1, 8, 512, 512]             136\n",
      "         conv_3_1-16          [-1, 8, 512, 512]               0\n",
      "        MaxPool2d-17          [-1, 8, 256, 256]               0\n",
      "           Conv2d-18         [-1, 16, 256, 256]           1,168\n",
      "      BatchNorm2d-19         [-1, 16, 256, 256]              32\n",
      "             ReLU-20         [-1, 16, 256, 256]               0\n",
      "           Conv2d-21         [-1, 16, 256, 256]           2,320\n",
      "      BatchNorm2d-22         [-1, 16, 256, 256]              32\n",
      "             ReLU-23         [-1, 16, 256, 256]               0\n",
      "     conv_block_3-24         [-1, 16, 256, 256]               0\n",
      "           Conv2d-25         [-1, 16, 256, 256]           6,288\n",
      "      BatchNorm2d-26         [-1, 16, 256, 256]              32\n",
      "             ReLU-27         [-1, 16, 256, 256]               0\n",
      "           Conv2d-28         [-1, 16, 256, 256]          12,560\n",
      "      BatchNorm2d-29         [-1, 16, 256, 256]              32\n",
      "             ReLU-30         [-1, 16, 256, 256]               0\n",
      "     conv_block_7-31         [-1, 16, 256, 256]               0\n",
      "           Conv2d-32         [-1, 16, 256, 256]             528\n",
      "         conv_3_1-33         [-1, 16, 256, 256]               0\n",
      "        MaxPool2d-34         [-1, 16, 128, 128]               0\n",
      "           Conv2d-35         [-1, 32, 128, 128]           4,640\n",
      "      BatchNorm2d-36         [-1, 32, 128, 128]              64\n",
      "             ReLU-37         [-1, 32, 128, 128]               0\n",
      "           Conv2d-38         [-1, 32, 128, 128]           9,248\n",
      "      BatchNorm2d-39         [-1, 32, 128, 128]              64\n",
      "             ReLU-40         [-1, 32, 128, 128]               0\n",
      "     conv_block_3-41         [-1, 32, 128, 128]               0\n",
      "           Conv2d-42         [-1, 32, 128, 128]          25,120\n",
      "      BatchNorm2d-43         [-1, 32, 128, 128]              64\n",
      "             ReLU-44         [-1, 32, 128, 128]               0\n",
      "           Conv2d-45         [-1, 32, 128, 128]          50,208\n",
      "      BatchNorm2d-46         [-1, 32, 128, 128]              64\n",
      "             ReLU-47         [-1, 32, 128, 128]               0\n",
      "     conv_block_7-48         [-1, 32, 128, 128]               0\n",
      "           Conv2d-49         [-1, 32, 128, 128]           2,080\n",
      "         conv_3_1-50         [-1, 32, 128, 128]               0\n",
      "        MaxPool2d-51           [-1, 32, 64, 64]               0\n",
      "           Conv2d-52           [-1, 64, 64, 64]          18,496\n",
      "      BatchNorm2d-53           [-1, 64, 64, 64]             128\n",
      "             ReLU-54           [-1, 64, 64, 64]               0\n",
      "           Conv2d-55           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-56           [-1, 64, 64, 64]             128\n",
      "             ReLU-57           [-1, 64, 64, 64]               0\n",
      "     conv_block_3-58           [-1, 64, 64, 64]               0\n",
      "           Conv2d-59           [-1, 64, 64, 64]         100,416\n",
      "      BatchNorm2d-60           [-1, 64, 64, 64]             128\n",
      "             ReLU-61           [-1, 64, 64, 64]               0\n",
      "           Conv2d-62           [-1, 64, 64, 64]         200,768\n",
      "      BatchNorm2d-63           [-1, 64, 64, 64]             128\n",
      "             ReLU-64           [-1, 64, 64, 64]               0\n",
      "     conv_block_7-65           [-1, 64, 64, 64]               0\n",
      "           Conv2d-66           [-1, 64, 64, 64]           8,256\n",
      "         conv_3_1-67           [-1, 64, 64, 64]               0\n",
      "        MaxPool2d-68           [-1, 64, 32, 32]               0\n",
      "           Conv2d-69          [-1, 128, 32, 32]          73,856\n",
      "      BatchNorm2d-70          [-1, 128, 32, 32]             256\n",
      "             ReLU-71          [-1, 128, 32, 32]               0\n",
      "           Conv2d-72          [-1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-73          [-1, 128, 32, 32]             256\n",
      "             ReLU-74          [-1, 128, 32, 32]               0\n",
      "     conv_block_3-75          [-1, 128, 32, 32]               0\n",
      "           Conv2d-76          [-1, 128, 32, 32]         401,536\n",
      "      BatchNorm2d-77          [-1, 128, 32, 32]             256\n",
      "             ReLU-78          [-1, 128, 32, 32]               0\n",
      "           Conv2d-79          [-1, 128, 32, 32]         802,944\n",
      "      BatchNorm2d-80          [-1, 128, 32, 32]             256\n",
      "             ReLU-81          [-1, 128, 32, 32]               0\n",
      "     conv_block_7-82          [-1, 128, 32, 32]               0\n",
      "           Conv2d-83          [-1, 128, 32, 32]          32,896\n",
      "         conv_3_1-84          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-85           [-1, 64, 64, 64]          32,832\n",
      "          up_conv-86           [-1, 64, 64, 64]               0\n",
      "           Conv2d-87           [-1, 64, 64, 64]          73,792\n",
      "      BatchNorm2d-88           [-1, 64, 64, 64]             128\n",
      "             ReLU-89           [-1, 64, 64, 64]               0\n",
      "           Conv2d-90           [-1, 64, 64, 64]          36,928\n",
      "      BatchNorm2d-91           [-1, 64, 64, 64]             128\n",
      "             ReLU-92           [-1, 64, 64, 64]               0\n",
      "     conv_block_3-93           [-1, 64, 64, 64]               0\n",
      "           Conv2d-94           [-1, 64, 64, 64]         401,472\n",
      "      BatchNorm2d-95           [-1, 64, 64, 64]             128\n",
      "             ReLU-96           [-1, 64, 64, 64]               0\n",
      "           Conv2d-97           [-1, 64, 64, 64]         200,768\n",
      "      BatchNorm2d-98           [-1, 64, 64, 64]             128\n",
      "             ReLU-99           [-1, 64, 64, 64]               0\n",
      "    conv_block_7-100           [-1, 64, 64, 64]               0\n",
      "          Conv2d-101           [-1, 64, 64, 64]           8,256\n",
      "        conv_3_1-102           [-1, 64, 64, 64]               0\n",
      " ConvTranspose2d-103         [-1, 32, 128, 128]           8,224\n",
      "         up_conv-104         [-1, 32, 128, 128]               0\n",
      "          Conv2d-105         [-1, 32, 128, 128]          18,464\n",
      "     BatchNorm2d-106         [-1, 32, 128, 128]              64\n",
      "            ReLU-107         [-1, 32, 128, 128]               0\n",
      "          Conv2d-108         [-1, 32, 128, 128]           9,248\n",
      "     BatchNorm2d-109         [-1, 32, 128, 128]              64\n",
      "            ReLU-110         [-1, 32, 128, 128]               0\n",
      "    conv_block_3-111         [-1, 32, 128, 128]               0\n",
      "          Conv2d-112         [-1, 32, 128, 128]         100,384\n",
      "     BatchNorm2d-113         [-1, 32, 128, 128]              64\n",
      "            ReLU-114         [-1, 32, 128, 128]               0\n",
      "          Conv2d-115         [-1, 32, 128, 128]          50,208\n",
      "     BatchNorm2d-116         [-1, 32, 128, 128]              64\n",
      "            ReLU-117         [-1, 32, 128, 128]               0\n",
      "    conv_block_7-118         [-1, 32, 128, 128]               0\n",
      "          Conv2d-119         [-1, 32, 128, 128]           2,080\n",
      "        conv_3_1-120         [-1, 32, 128, 128]               0\n",
      " ConvTranspose2d-121         [-1, 16, 256, 256]           2,064\n",
      "         up_conv-122         [-1, 16, 256, 256]               0\n",
      "          Conv2d-123         [-1, 16, 256, 256]           4,624\n",
      "     BatchNorm2d-124         [-1, 16, 256, 256]              32\n",
      "            ReLU-125         [-1, 16, 256, 256]               0\n",
      "          Conv2d-126         [-1, 16, 256, 256]           2,320\n",
      "     BatchNorm2d-127         [-1, 16, 256, 256]              32\n",
      "            ReLU-128         [-1, 16, 256, 256]               0\n",
      "    conv_block_3-129         [-1, 16, 256, 256]               0\n",
      "          Conv2d-130         [-1, 16, 256, 256]          25,104\n",
      "     BatchNorm2d-131         [-1, 16, 256, 256]              32\n",
      "            ReLU-132         [-1, 16, 256, 256]               0\n",
      "          Conv2d-133         [-1, 16, 256, 256]          12,560\n",
      "     BatchNorm2d-134         [-1, 16, 256, 256]              32\n",
      "            ReLU-135         [-1, 16, 256, 256]               0\n",
      "    conv_block_7-136         [-1, 16, 256, 256]               0\n",
      "          Conv2d-137         [-1, 16, 256, 256]             528\n",
      "        conv_3_1-138         [-1, 16, 256, 256]               0\n",
      " ConvTranspose2d-139          [-1, 8, 512, 512]             520\n",
      "         up_conv-140          [-1, 8, 512, 512]               0\n",
      "          Conv2d-141          [-1, 8, 512, 512]           1,160\n",
      "     BatchNorm2d-142          [-1, 8, 512, 512]              16\n",
      "            ReLU-143          [-1, 8, 512, 512]               0\n",
      "          Conv2d-144          [-1, 8, 512, 512]             584\n",
      "     BatchNorm2d-145          [-1, 8, 512, 512]              16\n",
      "            ReLU-146          [-1, 8, 512, 512]               0\n",
      "    conv_block_3-147          [-1, 8, 512, 512]               0\n",
      "          Conv2d-148          [-1, 8, 512, 512]           6,280\n",
      "     BatchNorm2d-149          [-1, 8, 512, 512]              16\n",
      "            ReLU-150          [-1, 8, 512, 512]               0\n",
      "          Conv2d-151          [-1, 8, 512, 512]           3,144\n",
      "     BatchNorm2d-152          [-1, 8, 512, 512]              16\n",
      "            ReLU-153          [-1, 8, 512, 512]               0\n",
      "    conv_block_7-154          [-1, 8, 512, 512]               0\n",
      "          Conv2d-155          [-1, 8, 512, 512]             136\n",
      "        conv_3_1-156          [-1, 8, 512, 512]               0\n",
      "          Conv2d-157          [-1, 1, 512, 512]               9\n",
      "================================================================\n",
      "Total params: 2,946,817\n",
      "Trainable params: 2,946,817\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 1045.50\n",
      "Params size (MB): 11.24\n",
      "Estimated Total Size (MB): 1057.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model_pytorch.to(device), (1,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mackr\\Desktop\\thesisProgram\\dip-dl-project\\segmentation\\Implementation_U_Net\\PytorchToTensorflow.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mackr/Desktop/thesisProgram/dip-dl-project/segmentation/Implementation_U_Net/PytorchToTensorflow.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dummy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray((\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m512\u001b[39m,\u001b[39m512\u001b[39m)))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mackr/Desktop/thesisProgram/dip-dl-project/segmentation/Implementation_U_Net/PytorchToTensorflow.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dummpy_output \u001b[39m=\u001b[39m model_pytorch(dummy_input)\n",
      "File \u001b[1;32mc:\\Users\\mackr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mackr\\Desktop\\thesisProgram\\dip-dl-project\\segmentation\\Implementation_U_Net\\MSUnet_Model.py:286\u001b[0m, in \u001b[0;36mMSU_Net.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 286\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mConv1(x)\n\u001b[0;32m    288\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMaxpool(x1)\n\u001b[0;32m    289\u001b[0m     x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mConv2(x2)\n",
      "File \u001b[1;32mc:\\Users\\mackr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mackr\\Desktop\\thesisProgram\\dip-dl-project\\segmentation\\Implementation_U_Net\\MSUnet_Model.py:163\u001b[0m, in \u001b[0;36mconv_3_1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    161\u001b[0m     \u001b[39m#x1 = self.conv_1(x)\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[39m#x2 = self.conv_2(x)\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_3(x)\n\u001b[0;32m    164\u001b[0m     \u001b[39m#x5 = self.conv_5(x)\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     x7 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_7(x)\n",
      "File \u001b[1;32mc:\\Users\\mackr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mackr\\Desktop\\thesisProgram\\dip-dl-project\\segmentation\\Implementation_U_Net\\MSUnet_Model.py:96\u001b[0m, in \u001b[0;36mconv_block_3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 96\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[0;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mackr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mackr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mackr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mackr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\mackr\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4]"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.from_numpy(np.array((1,1,512,512))).float().to(device)\n",
    "dummpy_output = model_pytorch(dummy_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
