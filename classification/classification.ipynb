{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from resnet50 import getResNet50\n",
    "from models import getClassReport as class_report\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "import keras\n",
    "from models import getConvNext, getInceptionV3, getEfficientNetB0, getROCScore\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# folder = '../data/augmented-data/U-Net/ODIR'\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'ODIR_AMD_w_others_pred.pickle','rb')\n",
    "# ODIR_AMD_w_others = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'ODIR_Pure_AMD_pred.pickle','rb')\n",
    "# ODIR_Pure_AMD = pickle.load(pickle_in)\n",
    "\n",
    "# folder = '../data/augmented-data/U-Net/RFMID'\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'RFMID_AMD_w_others_pred.pickle','rb')\n",
    "# RFMID_AMD_w_others = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'RFMID_Pure_AMD_pred.pickle','rb')\n",
    "# RFMID_Pure_AMD = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'RFMID_normal_pred.pickle','rb')\n",
    "# RFMID_normal = pickle.load(pickle_in)\n",
    "\n",
    "# folder = '../data/augmented-data/U-Net/STARE'\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'stare_AMD_w_others_pred.pickle','rb')\n",
    "# stare_AMD_w_others = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'stare_normal_pred.pickle','rb')\n",
    "# stare_normal = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archived\n",
    "# pickle_in = open(folder + '/' + 'ODIR_Normal_pred.pickle','rb')\n",
    "# ODIR_normal = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'RFMiD_AMD_w_others_testing_pred.pickle','rb')\n",
    "# RFMID_AMD_w_others_test = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'RFMiD_Pure_AMD_testing_pred.pickle','rb')\n",
    "# RFMID_Pure_AMD_test = pickle.load(pickle_in)\n",
    "\n",
    "# pickle_in = open(folder + '/' + 'RFMiD_normal_testing_pred.pickle','rb')\n",
    "# RFMID_normal_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLAHE Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "folder = '../data/augmented-data/CLAHE/ODIR'\n",
    "\n",
    "pickle_in = open(folder + '/' + 'ODIR_AMD_w_others_data.pickle','rb')\n",
    "ODIR_AMD_w_others = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(folder + '/' + 'ODIR_Pure_AMD_data.pickle','rb')\n",
    "ODIR_Pure_AMD = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(folder + '/' + 'ODIR_Normal_data.pickle','rb')\n",
    "ODIR_normal = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "folder = '../data/augmented-data/CLAHE/RFMID'\n",
    "\n",
    "pickle_in = open(folder + '/' + 'RFMiD_AMD_w_others_data.pickle','rb')\n",
    "RFMID_AMD_w_others = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(folder + '/' + 'RFMiD_Pure_AMD_data.pickle','rb')\n",
    "RFMID_Pure_AMD = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(folder + '/' + 'RFMiD_normal_data.pickle','rb')\n",
    "RFMID_normal = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "folder = '../data/augmented-data/CLAHE/STARE'\n",
    "\n",
    "pickle_in = open(folder + '/' + 'stare_AMD_w_others_data.pickle','rb')\n",
    "stare_AMD_w_others = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(folder + '/' + 'stare_normal_data.pickle','rb')\n",
    "stare_normal = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the image\n",
    "height, width, channels = ODIR_AMD_w_others[0].shape\n",
    "\n",
    "# Print the dimensions\n",
    "print(\"Width:\", width)\n",
    "print(\"Height:\", height)\n",
    "print(\"Channels:\", channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(stare_normal[1], interpolation='nearest', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_amd_mixed = np.concatenate((ODIR_Pure_AMD[:160], RFMID_AMD_w_others), axis=0) # Complex Data\n",
    "segmented_pure_amd = np.concatenate((ODIR_Pure_AMD, RFMID_Pure_AMD), axis=0) # Pure Data\n",
    "segmented_amd_w_others = np.concatenate((ODIR_AMD_w_others, RFMID_AMD_w_others, stare_AMD_w_others), axis=0) # Archived Complex\n",
    "\n",
    "segmented_complex = segmented_amd_w_others\n",
    "segmented_normal = RFMID_normal[:580] # if using only clahe, put limit = [:580]\n",
    "\n",
    "# segmented_amd_test = np.concatenate((RFMID_AMD_w_others_test, RFMID_Pure_AMD_test), axis=0)\n",
    "# segmented_normal_test = RFMID_normal_test[:138]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('amd:', len(segmented_complex))\n",
    "print('normal:', len(segmented_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_amd = []\n",
    "y_normal = []\n",
    "\n",
    "for i in range(len(segmented_complex)):\n",
    "    y_amd.append(1)\n",
    "\n",
    "for i in range(len(segmented_normal)):\n",
    "    y_normal.append(0)\n",
    "\n",
    "label_amd = np.array(y_amd)\n",
    "label_normal = np.array(y_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((segmented_complex, segmented_normal), axis=0)\n",
    "y = np.concatenate((label_amd, label_normal), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x size:', len(x))\n",
    "print('y size:', len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, alpha, y_train, omega = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "x_eval, x_test, y_eval, y_test = train_test_split(alpha, omega, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Input Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (512, 512, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model_filename = '../rendered-models/res-net-models/res_net_v7.h5'\n",
    "resnet_clahe_model_filename = '../rendered-models/res-net-models/res_net_clahe_v1.h5'\n",
    "\n",
    "inception_model_filename = '../rendered-models/inception-models/inception_v2.h5'\n",
    "inception_clahe_model_filename = '../rendered-models/inception-models/inception_clahe_v2.h5'\n",
    "\n",
    "cnn_model_filename = '../rendered-models/cnn-models/cnn_v2.h5'\n",
    "cnn_clahe_model_filename = '../rendered-models/cnn-models/cnn_clahe_v2.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "cnnModel = tf.keras.Sequential([\n",
    "    # First convolutional layer\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Second convolutional layer\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # Flatten the output of the convolutional layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # First fully connected layer\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resNetModel = getResNet50(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNext Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convNextModel = getConvNext(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception v3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionModel = getInceptionV3(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFficient Net v2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientNetModel = getEfficientNetB0(input_shape)\n",
    "adam = keras.optimizers.Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inceptionModel\n",
    "model_filename = inception_clahe_model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.0001)\n",
    "cnnModel.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.0001)\n",
    "inceptionModel.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.0001)\n",
    "resNetModel.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size and number of epochs for training\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "callback_checkpoint =[EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=False), # patience=10\n",
    "ModelCheckpoint(\n",
    "    model_filename,\n",
    "    verbose=1,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "),\n",
    "ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=15, min_lr=1e-7, verbose=1), # patience=8\n",
    "TensorBoard()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shapes:')\n",
    "print('x_train = ', x_train.shape)\n",
    "print('y_train = ', y_train.shape)\n",
    "print('x_eval = ', x_eval.shape)\n",
    "print('y_eval = ', y_eval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device(\"GPU\"):\n",
    "#     history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_eval, y_eval),\n",
    "#         callbacks=[callback_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model | Creating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_filename)\n",
    "with tf.device(\"GPU\"):\n",
    "    result = model.predict(x_test)\n",
    "\n",
    "threshold = 0.5\n",
    "predictions = np.where(result > threshold, 1, 0).flatten()\n",
    "print('Predictions: ', predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "threshold = 0.5\n",
    "# CNN\n",
    "cnnModel.load_weights(cnn_clahe_model_filename)\n",
    "with tf.device(\"GPU\"): \n",
    "    result = cnnModel.predict(x_test)\n",
    "result1 = np.where(result > threshold, 1, 0).flatten()\n",
    "# Inception\n",
    "inceptionModel.load_weights(inception_clahe_model_filename)\n",
    "with tf.device(\"GPU\"): \n",
    "    result = inceptionModel.predict(x_test)\n",
    "result2 = np.where(result > threshold, 1, 0).flatten()\n",
    "# ResNet\n",
    "resNetModel.load_weights(resnet_clahe_model_filename)\n",
    "with tf.device(\"GPU\"): \n",
    "    result = resNetModel.predict(x_test)\n",
    "result3 = np.where(result > threshold, 1, 0).flatten()\n",
    "\n",
    "# Calculate ROC curves and AUC for each model\n",
    "fpr_model1, tpr_model1, thresholds_model1 = roc_curve(y_test, result1)\n",
    "roc_auc_model1 = auc(fpr_model1, tpr_model1)\n",
    "\n",
    "fpr_model2, tpr_model2, thresholds_model2 = roc_curve(y_test, result2)\n",
    "roc_auc_model2 = auc(fpr_model2, tpr_model2)\n",
    "\n",
    "fpr_model3, tpr_model3, thresholds_model3 = roc_curve(y_test, result3)\n",
    "roc_auc_model3 = auc(fpr_model3, tpr_model3)\n",
    "\n",
    "# Plot the ROC curves for all three models\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_model1, tpr_model1, color='b', lw=2, label=f'CNN (AUC = {roc_auc_model1:.2f})')\n",
    "plt.plot(fpr_model2, tpr_model2, color='g', lw=2, label=f'Inception (AUC = {roc_auc_model2:.2f})')\n",
    "plt.plot(fpr_model3, tpr_model3, color='r', lw=2, label=f'ResNet (AUC = {roc_auc_model3:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    # Convert the NumPy array to an HTML image tag\n",
    "    img_tag = f'<img src=\"data:image/png;base64,{image.tostring()}\" />'\n",
    "    return img_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "image_tags = [f'<img src=\"data:image/png;base64,{image.tobytes()}\" />' for image in x_test]\n",
    "data = { 'Data': image_tags, 'Actual': y_test, 'Predicted': predictions }\n",
    "preds = pd.DataFrame(data)\n",
    "display_preds = preds.to_html(escape=False)\n",
    "\n",
    "display(HTML(display_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_rows, num_cols = 3, 3\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "testData = x_test[15:]\n",
    "\n",
    "for i, (image, ax) in enumerate(zip(testData, axes)):\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Actual = {} | Predicted = {}'.format('Normal' if y_test[i+15] == 0 else 'AMD', 'Normal' if predictions[i+15] == 0 else 'AMD'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_pred = (preds['Actual'] == preds['Predicted']).sum()\n",
    "print(f'{correctly_pred} out of {len(preds)} were correctly predicted')\n",
    "print(f'{(correctly_pred/len(preds))*100}% is your score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpos = 0\n",
    "tneg = 0\n",
    "fpos = 0\n",
    "fneg = 0\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if preds['Actual'][i] == 1 and preds['Predicted'][i] == 1:\n",
    "        tpos+=1\n",
    "    elif preds['Actual'][i] == 1 and preds['Predicted'][i] == 0:\n",
    "        fneg+=1\n",
    "    elif preds['Actual'][i] == 0 and preds['Predicted'][i] == 0:\n",
    "        tneg+=1\n",
    "    elif preds['Actual'][i] == 0 and preds['Predicted'][i] == 1:\n",
    "        fpos+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = (tpos/(tpos+fpos))\n",
    "recall = (tpos/(tpos+fneg))\n",
    "accuracy = ((tpos+tneg)/(tpos+tneg+fpos+fneg))\n",
    "fone = 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: {}%'.format(round(precision*100, 2)))\n",
    "print('Recall: {}%'.format(round(recall*100, 2)))\n",
    "print('Accuracy: {}%'.format(round(accuracy*100, 2)))\n",
    "print('F1-Score: {}%'.format(round(fone*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import getConfusionMatrix, getAccuracyVisual, getLossVisual\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "getConfusionMatrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('true positive: ', tpos)\n",
    "print('false positive: ', fpos)\n",
    "print('true negative: ', tneg)\n",
    "print('false negative: ', fneg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
